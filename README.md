# End-to-end data pipeline using Databricks
Pipeline steps:

1. Use Databricks features to explore a raw dataset.

2. Create a Databricks notebook to ingest raw source data and write the raw data to a target table.

3. Create a Databricks notebook to transform the raw source data and write the transformed data to a target table.

4. Create a Databricks notebook to query the transformed data.

5. Automate the data pipeline with a Databricks job.

## Cluster



## Notebooks
Scripts are available in this repo.


## Catalog


## Workflow job



## Trigger schedule



## Output